{"cells":[{"cell_type":"code","metadata":{"graffitiCellId":"id_3xxr5fb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B116BFDF0D464FF49A85A582357D0B4D","mdEditEnable":false,"collapsed":false,"scrolled":false},"source":"# 线性回归\n主要内容包括：\n\n1. 线性回归的基本要素\n2. 线性回归模型从零开始的实现\n3. 线性回归模型使用pytorch的简洁实现","outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid character in identifier (<ipython-input-1-af033d0f4323>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-af033d0f4323>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    主要内容包括：\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"]}],"execution_count":1},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ht8ukap","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8FCA1BC77B7F479BA1398473C2691BB0","mdEditEnable":false},"source":"## 线性回归的基本要素\n\n### 模型\n为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:\n\n\n$$\n\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b\n$$\n\n\n\n### 数据集\n我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。\n### 损失函数\n在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为\n\n\n$$\nl^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,\n$$\n\n\n\n$$\nL(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.\n$$\n\n\n### 优化函数 - 随机梯度下降\n当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。\n\n在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   \n\n$$\n(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)\n$$\n  \n学习率: $\\eta$代表在每次优化中，能够学习的步长的大小    \n批量大小: $\\mathcal{B}$是小批量计算中的批量大小batch size   \n\n总结一下，优化函数的有以下两个步骤：\n\n- (i)初始化模型参数，一般来说使用随机初始化；\n- (ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_v3gyr0b","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"469D697FF90B48B7B0B61AED429EB8D6","mdEditEnable":false},"source":"## 矢量计算\n在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。\n\n\n1. 向量相加的一种方法是，将这两个向量按元素逐一做标量加法。\n2. 向量相加的另一种方法是，将这两个向量直接做矢量加法。"},{"cell_type":"code","execution_count":2,"metadata":{"graffitiCellId":"id_bp6luds","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"631AD2C3EA1A431287E30A95D535D877","collapsed":false,"scrolled":false},"outputs":[],"source":"import torch\nimport time\n\n# init variable a, b as 1000 dimension vector\nn = 1000\na = torch.ones(n)\nb = torch.ones(n)\n"},{"metadata":{"id":"3B2D835028AC4F0587CE096382065C76","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([1000])"},"transient":{},"execution_count":5}],"source":"a.size()","execution_count":5},{"metadata":{"id":"039D90BFE4BF48939FA0D06FC64699D2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([1000, 1])\n","name":"stdout"}],"source":"a_1=a.view(a.size(0),-1)\nprint(a_1.size())","execution_count":9},{"cell_type":"code","execution_count":4,"metadata":{"graffitiCellId":"id_xxj5nbf","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"55B0FCA128314322808F46633FA9B944","collapsed":false,"scrolled":false},"outputs":[],"source":"# define a timer class to record time\nclass Timer(object):\n    \"\"\"Record multiple running times.\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start()\n\n    def start(self):\n        # start the timer\n        self.start_time = time.time()\n\n    def stop(self):\n        # stop the timer and record time into a list\n        self.times.append(time.time() - self.start_time)\n        return self.times[-1]\n\n    def avg(self):\n        # calculate the average and return\n        return sum(self.times)/len(self.times)\n\n    def sum(self):\n        # return the sum of recorded time\n        return sum(self.times)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_g9h7dg8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2698821CF46844989522D09B8B1C76DB","mdEditEnable":false},"source":"现在我们可以来测试了。首先将两个向量使用for循环按元素逐一做标量加法。"},{"cell_type":"code","execution_count":5,"metadata":{"graffitiCellId":"id_eoz706b","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DF2AACFBA2EA42698CC82C33AF79AEDB","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'0.01142 sec'"},"transient":{},"execution_count":5}],"source":"timer = Timer()\nc = torch.zeros(n)\nfor i in range(n):\n    c[i] = a[i] + b[i]\n'%.5f sec' % timer.stop()"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_akkwkh8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B00F06B72BB5471DA82C945B04FED140","mdEditEnable":false},"source":"另外是使用torch来将两个向量直接做矢量加法："},{"cell_type":"code","execution_count":6,"metadata":{"graffitiCellId":"id_a8sw68j","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6D2503874A514A7590AF8F710B5F325C","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'0.00022 sec'"},"transient":{},"execution_count":6}],"source":"timer.start()\nd = a + b\n'%.5f sec' % timer.stop()"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_oonn3xx","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B0CA3D998E0A4B5C848F9C1BAC37DB13","mdEditEnable":false},"source":"结果很明显,后者比前者运算速度更快。因此，我们应该尽可能采用矢量计算，以提升计算效率。"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3y8h3t7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"84D91561397548D7ACB5FAB71E66AB9B","mdEditEnable":false},"source":"## 线性回归模型从零开始的实现\n\n"},{"cell_type":"code","execution_count":2,"metadata":{"graffitiCellId":"id_3snj2zc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B3148881D9514B898929430997FD781C","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"# import packages and modules\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\n\nprint(torch.__version__)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ofruiuq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D7C96AC35B12411E8A1530B965CB34E0","mdEditEnable":false},"source":"### 生成数据集\n使用线性模型来生成数据集，生成一个1000个样本的数据集，下面是用来生成数据的线性关系：\n\n$$\n\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b\n$$\n\n"},{"cell_type":"code","execution_count":4,"metadata":{"graffitiCellId":"id_h3bosrm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1A5F9ED7F99643A3A440960077439F0F","collapsed":false,"scrolled":false},"outputs":[],"source":"# set input feature number \nnum_inputs = 2\n# set example number\nnum_examples = 1000\n\n# set true weight and bias in order to generate corresponded label\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nfeatures = torch.randn(num_examples, num_inputs,\n                      dtype=torch.float32)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n                       dtype=torch.float32)#  list, tuple, array, scalar 转化为tensor"},{"metadata":{"id":"F5FC602276D2469880E69F1E73AD8B00","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"tensor_1=torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n                       dtype=torch.float32)             #   \n                       ","execution_count":5},{"metadata":{"id":"14B3B7A21316406788D478054DFB0D59","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([1000])\n","name":"stdout"}],"source":"print(tensor_1.size())","execution_count":6},{"metadata":{"id":"B3FC56F95D2A4C3888CAD56DEAA0C03C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([1000, 2])"},"transient":{},"execution_count":7}],"source":"features.size()","execution_count":7},{"metadata":{"id":"10E43FD0F8A04A2CAD01F8563B1D30DE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([1000])"},"transient":{},"execution_count":8}],"source":"labels.size()","execution_count":8},{"metadata":{"id":"6B84CCAFC0CA4E1894426176AD1657B1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([1000])"},"transient":{},"execution_count":9}],"source":"labels.size()","execution_count":9},{"metadata":{"id":"E7B9F80A66DB490E996C885386C4DF50","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([1000])"},"transient":{},"execution_count":10}],"source":"features[:,0].size()","execution_count":10},{"metadata":{"id":"CBF8BA48282542018687DE5D887FD12F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"2"},"transient":{},"execution_count":11}],"source":"true_w[0]","execution_count":11},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_gr10soh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"937B9B59AC2343B58488AAA9B7C11C2A","mdEditEnable":false},"source":"### 使用图像来展示生成的数据"},{"cell_type":"code","execution_count":13,"metadata":{"graffitiCellId":"id_ov2af2a","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8E2E1E16060241C6A33E4CF1EC65DF1D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8E2E1E16060241C6A33E4CF1EC65DF1D/q5n65ipclk.png\">"},"transient":{}}],"source":"plt.scatter(features[:, 1].numpy(), labels.numpy(), 1);  # tensor.numpy()表示将tensor 转化为Numpy"},{"metadata":{"id":"B50456E915074D2A9FCA5592776E487A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[ 0.8877,  0.2456],\n        [-1.0391,  0.7241],\n        [-0.5199, -0.0042],\n        ...,\n        [ 0.5400, -0.3943],\n        [-0.7943,  0.3329],\n        [-0.3577, -0.6104]])\n1000\n2\n","name":"stdout"}],"source":"print(features)   #   .size(0) 表示行数  .size(1)表示列数\nprint(features.size(0))\nprint(features.size(1))","execution_count":12},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_iivzo2j","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"27981A0FD4054AC39194415A90F313EC","mdEditEnable":false},"source":"### 读取数据集"},{"cell_type":"code","execution_count":13,"metadata":{"graffitiCellId":"id_0tj7eus","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A6E1419DA00C4ABF8CBF0E0F0B2B9E35","collapsed":false,"scrolled":false},"outputs":[],"source":"def data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)  # 打乱indices 这个list 里面的数字的顺序\n    for i in range(0, num_examples, batch_size):  #取0,10,20,,,990\n        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch\n        yield  features.index_select(0, j), labels.index_select(0, j) \n        #  返回一个迭代器包含分别features和lables里指定的行构成的tensor ,行索引在j 中"},{"metadata":{"id":"0F79684FD4CA4A84BA5EB8D833B0FD8B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([833, 393, 418,  61, 327, 155, 165, 890,  39, 168])\ntensor([[-2.0612, -0.9871],\n        [ 1.8983,  1.4066],\n        [-0.0882, -1.2238],\n        [ 1.7690,  0.7598],\n        [-0.2678, -0.3951],\n        [ 0.2469,  0.2984],\n        [ 1.6884,  0.9489],\n        [ 1.5071, -0.9317],\n        [-1.3891, -0.5439],\n        [ 0.3203,  0.7078]])\ntensor([ 3.4173,  3.2047,  8.1748,  5.1670,  5.0062,  3.6648,  4.3612, 10.3912,\n         3.2692,  2.4205])\n","name":"stdout"}],"source":"#  弄懂data_iter函数过程\nbatch_size_1=10\nnum_examples_1=1000\nindices_1=list(range(num_examples_1))\nrandom.shuffle(indices_1)\nfor i_1 in range(0, 1000, 10):\n    j_1 = torch.LongTensor(indices_1[i_1: min(i_1 + batch_size_1, num_examples_1)]) # the last time may be not enough for a whole batch\n    if i_1==0:\n        print(j_1)\n        print(features.index_select(0, j_1))\n        print( labels.index_select(0, j_1))\n    # yield  features.index_select(0, j), labels.index_select(0, j)","execution_count":16},{"cell_type":"code","execution_count":17,"metadata":{"graffitiCellId":"id_xc0arq3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1DA3BC30E43E4F76970F712D89BDBC4D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tensor([[-1.0281, -0.3254],\n        [ 0.1331, -1.0631],\n        [-3.2566, -0.1795],\n        [ 1.3423,  1.1218],\n        [ 0.0078, -0.0416],\n        [-2.8044,  0.3354],\n        [ 1.2262, -0.9178],\n        [ 0.3653,  1.1076],\n        [ 0.2707, -0.3150],\n        [ 0.6833, -0.9698]]) \n tensor([ 3.2448,  8.0680, -1.6894,  3.0663,  4.3628, -2.5552,  9.7673,  1.1754,\n         5.8162,  8.8429])\n","name":"stdout"}],"source":"batch_size = 10\n\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break"},{"metadata":{"id":"07E1BE0F6160460590CD8B0551E1909B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([10, 2])"},"transient":{},"execution_count":28}],"source":"X.shape","execution_count":28},{"metadata":{"id":"DBE2DD279E7546199B008E7B2A19810A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([10])"},"transient":{},"execution_count":27}],"source":"y.shape","execution_count":27},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_hj6sxxx","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1FF819B45B1F44C88012EBB266C10EE8","mdEditEnable":false},"source":"### 初始化模型参数"},{"cell_type":"code","execution_count":18,"metadata":{"graffitiCellId":"id_g06bzki","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6B11AC0E574140CD9C2E722B05D0049D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([0.], requires_grad=True)"},"transient":{},"execution_count":18}],"source":"w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)\nb = torch.zeros(1, dtype=torch.float32)\n\nw.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)"},{"metadata":{"id":"C3A38DF46B6F484D8977A2AF49EA15E0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"array([[0.00599015],\n       [0.01168193]])"},"transient":{},"execution_count":26}],"source":"np.random.normal(0, 0.01, (num_inputs,1))","execution_count":26},{"metadata":{"id":"AE6AE3BFB6494752B03A11D59AEF6586","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([0.], requires_grad=True)\n","name":"stdout"}],"source":"print(b)","execution_count":27},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_zvsctyc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A91414B8FDF24835A06B6ADFAEC2C15C","mdEditEnable":false},"source":"### 定义模型\n定义用来训练参数的训练模型：\n\n$$\n\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b\n$$\n\n"},{"cell_type":"code","execution_count":28,"metadata":{"graffitiCellId":"id_l8xu5kf","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8DFF5BDD78884936899E3CE720BEEE3C","collapsed":false,"scrolled":false},"outputs":[],"source":"def linreg(X, w, b):\n    return torch.mm(X, w) + b"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_1sta0nq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C9B747281D1842C682F2AEB1F38B959D","mdEditEnable":false},"source":"### 定义损失函数\n我们使用的是均方误差损失函数：\n$$\nl^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,\n$$\n"},{"cell_type":"code","execution_count":29,"metadata":{"graffitiCellId":"id_r9p6ncn","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"58A55DD7B46842578BEA1A8689456B1A","collapsed":false,"scrolled":false},"outputs":[],"source":"def squared_loss(y_hat, y): \n    return (y_hat - y.view(y_hat.size())) ** 2 / 2"},{"metadata":{"id":"68EA96D8F88E43AAA8025EA7FF740BF3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([0, 1, 2])\ntensor([[0],\n        [1],\n        [2]])\ntensor([0, 1, 2])\ntensor([0, 1, 2])\ntensor([[0],\n        [1],\n        [2]])\ntensor([0, 1, 2])\ntensor([[2],\n        [3],\n        [4]])\ntensor([[0, 1, 2]])\ntensor([[2, 3, 4],\n        [3, 4, 5],\n        [4, 5, 6]])\n","name":"stdout"}],"source":"x_1=torch.arange(3)\nx_1_0=x_1.view(3)\nx_1_0_0=x_1.view(-1)  # 全部摊平得到一维数组，可以看成列向量\nx_1_1=x_1.view(3,-1)   #  要得到二维数组 且行数是三行，第二个参数-1 是自适应\nx_1_2=x_1_1.view(-1)\ny_1=torch.arange(2,5).view(3,1)\nz_1=torch.arange(3).view(1,3)\nprint(x_1)\nprint(x_1.view(y_1.shape))\nprint(x_1_0)\nprint(x_1_0_0)\nprint(x_1_1)\nprint(x_1_2)\nprint(y_1)\nprint(z_1)\nprint(x_1+y_1) #  相当于用x_1 的每一个元素加上y_1 的所有元素","execution_count":48},{"metadata":{"id":"8A8D9349369F40F09F79C3DA766D144A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([3])"},"transient":{},"execution_count":31}],"source":"x_1.shape","execution_count":31},{"metadata":{"id":"EFD8CFD3EA124A3C8A51D919D4CD154C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([3, 1])"},"transient":{},"execution_count":47}],"source":"y_1.shape","execution_count":47},{"metadata":{"id":"A4F7A753C46F4810AF9EACB066648ED6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([1, 3])"},"transient":{},"execution_count":33}],"source":"z_1.shape","execution_count":33},{"metadata":{"id":"79D4295B415F4C779E83B106CEBB4C07","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([3, 4, 5, 6, 7, 8])\ntensor([[3, 4],\n        [5, 6],\n        [7, 8]])\ntensor([[3, 4],\n        [5, 6],\n        [7, 8]])\ntensor([[3, 4],\n        [5, 6],\n        [7, 8]])\ntensor([3, 4, 5, 6, 7, 8])\ntensor([[3],\n        [4],\n        [5],\n        [6],\n        [7],\n        [8]])\ntensor([[3, 4, 5, 6, 7, 8]])\n","name":"stdout"}],"source":"n_1=torch.arange(3,9)\nn_2=n_1.view(3,-1)\nn_3=n_1.view(n_2.size())\nn_4=n_1.view(n_2.shape)\nn_5=n_4.view(-1)\nn_6=n_1.view(n_1.size(0),-1)\nn_7=n_1.view(-1,n_1.size(0))\nprint(n_1)\nprint(n_2)\nprint(n_3)\nprint(n_4)\nprint(n_5)\nprint(n_6)\nprint(n_7)","execution_count":4},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_jm7ie9i","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0A98B83A8FFD4E84B6EFE8A894643634","mdEditEnable":false},"source":"### 定义优化函数\n在这里优化函数使用的是小批量随机梯度下降：\n\n$$\n(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)\n$$\n  "},{"cell_type":"code","execution_count":32,"metadata":{"graffitiCellId":"id_e41t41x","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E9676D1B4F80473B894A4ADA3691D2E0","collapsed":false,"scrolled":false},"outputs":[],"source":"def sgd(params, lr, batch_size): \n    for param in params:\n        param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_0nsokgo","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B18F2D19AA1140478E2E327ECC97F40F","mdEditEnable":false},"source":"### 训练\n当数据集、模型、损失函数和优化函数定义完了之后就可来准备进行模型的训练了。\n"},{"cell_type":"code","execution_count":36,"metadata":{"graffitiCellId":"id_ht68g0d","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8C7AA862EE5A4AEAB3CB980F15870D06","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, loss 0.000050\nepoch 2, loss 0.000050\nepoch 3, loss 0.000050\nepoch 4, loss 0.000050\nepoch 5, loss 0.000051\n","name":"stdout"}],"source":"# super parameters init\nlr = 0.03\nnum_epochs = 5\n\nnet = linreg\nloss = squared_loss\n\n# training\nfor epoch in range(num_epochs):  # training repeats num_epochs times\n    # in each epoch, all the samples in dataset will be used once\n    \n    # X is the feature and y is the label of a batch sample\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y).sum()  \n        # calculate the gradient of batch sample loss \n        l.backward()  \n        # using small batch random gradient descent to iter model parameters\n        sgd([w, b], lr, batch_size)  \n        # reset parameter gradient\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n    train_l = loss(net(features, w, b), labels)\n    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))"},{"cell_type":"code","execution_count":30,"metadata":{"graffitiCellId":"id_6t702dg","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2E791A3F92EF4CCF91E2096630C0E8D9","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(tensor([[ 0.0056],\n         [-0.0103]], requires_grad=True),\n [2, -3.4],\n tensor([0.], requires_grad=True),\n 4.2)"},"transient":{},"execution_count":30}],"source":"w, true_w, b, true_b"},{"metadata":{"id":"9482D0919F67499F87EB4CD377F5D8B1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[0.],\n        [0.]])"},"transient":{},"execution_count":34}],"source":"w.grad","execution_count":34},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_pi6pxp6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7E8D79B69557446883330AB1E8DE07E2","mdEditEnable":false},"source":"## 线性回归模型使用pytorch的简洁实现\n"},{"cell_type":"code","execution_count":16,"metadata":{"graffitiCellId":"id_sdic11w","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D5CCF3AE67794558930978F1815C38B9","collapsed":false,"scrolled":false},"outputs":[],"source":"import torch\nfrom torch import nn\nimport numpy as np\ntorch.manual_seed(1)\n\nprint(torch.__version__)\ntorch.set_default_tensor_type('torch.FloatTensor')"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_07nlorv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"34B9AE6FB3D64DFD83E93D5CEF9EEE65","mdEditEnable":false},"source":"### 生成数据集\n在这里生成数据集跟从零开始的实现中是完全一样的。"},{"cell_type":"code","execution_count":17,"metadata":{"graffitiCellId":"id_k7z5rd0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"83C2DB9468394624BB4934DBF194A353","collapsed":false,"scrolled":false},"outputs":[],"source":"num_inputs = 2\nnum_examples = 1000\n\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nfeatures = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_io6yz0p","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0FB74CD3CD784A82B2A422E54BB0DEDD","mdEditEnable":false},"source":"### 读取数据集"},{"cell_type":"code","execution_count":18,"metadata":{"graffitiCellId":"id_bxmqh9f","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8704CA375BF04440839AB16AA995E3AB","collapsed":false,"scrolled":false},"outputs":[],"source":"import torch.utils.data as Data\n\nbatch_size = 10\n\n# combine featues and labels of dataset\ndataset = Data.TensorDataset(features, labels)\n\n# put dataset into DataLoader\ndata_iter = Data.DataLoader(\n    dataset=dataset,            # torch TensorDataset format\n    batch_size=batch_size,      # mini batch size\n    shuffle=True,               # whether shuffle the data or not\n    num_workers=2,              # read data in multithreading\n)"},{"cell_type":"code","execution_count":19,"metadata":{"graffitiCellId":"id_nnjw15x","scrolled":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C1FFC0FD8F5741D78AFD26B883BE192C","collapsed":false},"outputs":[],"source":"for X, y in data_iter:\n    print(X, '\\n', y)\n    break"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_zobpfwu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F9085AAAB3BB45E289329A5EA5446848","mdEditEnable":false},"source":"### 定义模型"},{"cell_type":"code","execution_count":20,"metadata":{"graffitiCellId":"id_gxy6vho","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"28DD8C6981314D148B5FD1915639151C","collapsed":false,"scrolled":false},"outputs":[],"source":"class LinearNet(nn.Module):\n    def __init__(self, n_feature):\n        super(LinearNet, self).__init__()      # call father function to init \n        self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`\n\n    def forward(self, x):\n        y = self.linear(x)\n        return y\n    \nnet = LinearNet(num_inputs)\nprint(net)"},{"cell_type":"code","execution_count":21,"metadata":{"graffitiCellId":"id_q5pjt1j","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"56CADFC7B65448BC989411C2C9950816","collapsed":false,"scrolled":false},"outputs":[],"source":"# ways to init a multilayer network\n# method one\nnet = nn.Sequential(\n    nn.Linear(num_inputs, 1)\n    # other layers can be added here\n    )\n\n# method two\nnet = nn.Sequential()\nnet.add_module('linear', nn.Linear(num_inputs, 1))\n# net.add_module ......\n\n# method three\nfrom collections import OrderedDict\nnet = nn.Sequential(OrderedDict([\n          ('linear', nn.Linear(num_inputs, 1))\n          # ......\n        ]))\n\nprint(net)\nprint(net[0])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_fl434p3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1BE602743BCD4C5D948A24212760162D","mdEditEnable":false},"source":"### 初始化模型参数"},{"cell_type":"code","execution_count":22,"metadata":{"graffitiCellId":"id_zdl7vmt","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"025B064D1ED1432385DEE75240A790F6","collapsed":false,"scrolled":false},"outputs":[],"source":"from torch.nn import init\n\ninit.normal_(net[0].weight, mean=0.0, std=0.01)\ninit.constant_(net[0].bias, val=0.0)  # or you can use `net[0].bias.data.fill_(0)` to modify it directly"},{"cell_type":"code","execution_count":23,"metadata":{"graffitiCellId":"id_7s9m78k","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C6A909A717B545E6802264EBD711588D","collapsed":false,"scrolled":false},"outputs":[],"source":"for param in net.parameters():\n    print(param)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_l729glu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BBFF587F757A4C7EB49AD0D536AD363E","mdEditEnable":false},"source":"### 定义损失函数"},{"cell_type":"code","execution_count":24,"metadata":{"graffitiCellId":"id_or1wah4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B721F8DD4811434BB1984B5B2DABC143","collapsed":false,"scrolled":false},"outputs":[],"source":"loss = nn.MSELoss()    # nn built-in squared loss function\n                       # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_zyt512e","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6490FA20F3D4462CB2B98902F694E525","mdEditEnable":false},"source":"### 定义优化函数"},{"cell_type":"code","execution_count":25,"metadata":{"graffitiCellId":"id_pmx4gbq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1998CEB53B534F178AC6223011627B0B","collapsed":false,"scrolled":false},"outputs":[],"source":"import torch.optim as optim\n\noptimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent function\nprint(optimizer)  # function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_n2klgfl","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"090AC5BD4E214B75BD7C4AB9B68720D0","mdEditEnable":false},"source":"### 训练"},{"cell_type":"code","execution_count":26,"metadata":{"graffitiCellId":"id_qj2fl3l","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A4B0F83F71F94728811A619F1AE74CD2","collapsed":false,"scrolled":false},"outputs":[],"source":"num_epochs = 3\nfor epoch in range(1, num_epochs + 1):\n    for X, y in data_iter:\n        output = net(X)\n        l = loss(output, y.view(-1, 1))\n        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()\n        l.backward()\n        optimizer.step()\n    print('epoch %d, loss: %f' % (epoch, l.item()))"},{"cell_type":"code","execution_count":27,"metadata":{"graffitiCellId":"id_ke4hsr4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"704087439A114181B3A7FE79539127AB","collapsed":false,"scrolled":false},"outputs":[],"source":"# result comparision\ndense = net[0]\nprint(true_w, dense.weight.data)\nprint(true_b, dense.bias.data)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_v7cg0i4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A968DC29635C4CDF8394A6F779661DC5","mdEditEnable":false},"source":"## 两种实现方式的比较\n1. 从零开始的实现（推荐用来学习）\n\n   能够更好的理解模型和神经网络底层的原理\n   \n\n2. 使用pytorch的简洁实现\n\n   能够更加快速地完成模型的设计与实现\n   "}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}